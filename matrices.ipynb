{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Get tf-idf_SVD, euclidean-distance and cosine-distance matrices\n",
    "The overall goal for this script is to iterate through **neuroscience-related R01s** for **each year**, get the abstracts for each of those projects and return three matrices and one dataframe:\n",
    "1. **tf-idf_SVD**: This is a SVD-decomposed tf-idf wordcount matrix [n_abstracts, n_components]. I will use this for k-means clustering and affinity propagation (I think).\n",
    "2. **cos_SVD**: This is a cosine distance matrix [n_abstracts, n_abstracts] generated from tf-idf_SVD. I will use this for Ward hierarchical clustering.\n",
    "3. **euc_SVD**: This is an euclidean distance matrix [n_abstracts, n_abstracts] generated from tf-idf_SVD. I will also use this for Ward hierarchical clustering.\n",
    "4. **df_neuro**: This is a dataframe, filtered for neuro-related R01s, containing information about each project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot\n",
    "\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Read abstracts and project csvs into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_abstracts_projects(year):\n",
    "    \"\"\"\n",
    "    input: int -- year to analyze; [1985:2016] inclusive.\n",
    "    returns: 2 dataframes -- project & abstract\n",
    "    \"\"\"\n",
    "    abs_dir = './abstracts/'\n",
    "    proj_dir = './projects/'\n",
    "    \n",
    "    project = pd.read_csv(proj_dir + 'RePORTER_PRJ_C_FY' + year + '.csv', encoding = \"ISO-8859-1\")\n",
    "    abstract = pd.read_csv(abs_dir + 'RePORTER_PRJABS_C_FY' + year + '.csv',encoding = \"ISO-8859-1\")\n",
    "    \n",
    "    return project, abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Clustering NIH data by common keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Clean abstracts (stemming, lowercase, no punctuation, remove stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     2,
     13
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get customized stopwords:\n",
    "def customized_stopwords(to_append=[]):\n",
    "    \"\"\"\n",
    "    to_append: list; what words do you want to exclude from your analysis, in addition to the standard \n",
    "    stopwords like 'the', 'and, 'of', and so on? See above to_append variable for examples.\n",
    "    returns: list; stopwords including to_append list\n",
    "    \"\"\"\n",
    "    stop = stopwords.words('english')\n",
    "    stop = stop + to_append\n",
    "    return stop\n",
    "\n",
    "# get list of words that are lowercase, with punctuation removed and words stemmed.\n",
    "def get_wordlist(abstract, stop = customized_stopwords()):\n",
    "    \"\"\"\n",
    "    returns a list of lowercase words from abstract with punctuation and stopwords removed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # make words lowercase\n",
    "        words = abstract.lower()\n",
    "        \n",
    "        # take out all punctuation and split strings into a list of words\n",
    "        words = (''.join(c for c in words if c not in punctuation)).split(' ')\n",
    "        \n",
    "        # remove stopwords\n",
    "        words = [\" \".join([w for w in word.split() if not w in stop]) for word in words]\n",
    "        \n",
    "        # stem words using Porter's Stemmer\n",
    "        stemmed = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                word = PorterStemmer().stem(word)\n",
    "            except IndexError:\n",
    "                word = word\n",
    "            if word != '' and word.isalpha():\n",
    "                stemmed.append(word)\n",
    "        words = stemmed\n",
    "\n",
    "    except AttributeError:\n",
    "        words = []\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Filter for neuro-related abstracts.\n",
    "I defined a project to be neuroscience-related if the abstract mentioned \"brain\" or \"neur*\" at least twice every 100 non-stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def neuro_count(row):\n",
    "    try:\n",
    "        return (row.ABSTRACT_TEXT.count(' brain') + row.ABSTRACT_TEXT.count('neur'))\n",
    "    except AttributeError:\n",
    "        return 0\n",
    "    \n",
    "def wordlist_count(row):\n",
    "    return len(row.wordlist)\n",
    "def neuro_only(df, word_density=0.02):\n",
    "    \"\"\"\n",
    "    input: dataframe\n",
    "    word_density: how many neuro related words for every 100 words that are not stopwords in an abstract? Stopwords: the, and, or, not, etc.\n",
    "    returns: dataframe containing neuro-related projects as defined above, with a column containing cleaned abstract keywords for analysis.\n",
    "    \"\"\"\n",
    "    df['abs_neuro_count'] = df.apply(neuro_count, axis=1)\n",
    "    df['wordlist'] = df.ABSTRACT_TEXT.apply(get_wordlist)\n",
    "    df['wordlist_ct'] = df.apply(wordlist_count, axis=1)\n",
    "    df['rel_neuro_count'] = df.abs_neuro_count / df.wordlist_ct\n",
    "    \n",
    "    # drop duplicates - I found that in some years, there are duplicate abstracts which really screwed up my hierarchical clustering\n",
    "    df = df.drop_duplicates('ABSTRACT_TEXT')\n",
    "    return df[df.rel_neuro_count >= word_density]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Get feature vectors into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def unique_words(df):\n",
    "    \"\"\"\n",
    "    input: df containing 'wordlist' column\n",
    "    returns: a dictionary containing unique words as keys, where dict[key][0] is total count of word over df\n",
    "    \"\"\"\n",
    "    all_words = list(df.wordlist)\n",
    "    all_words_list = [item for sublist in all_words for item in sublist]\n",
    "\n",
    "    count_dict = {}\n",
    "    \n",
    "    # Get total word count for each word in dataset\n",
    "    for word in all_words_list:\n",
    "        if word not in count_dict.keys():\n",
    "            count_dict[word] = [1]\n",
    "        else:\n",
    "            count_dict[word][0] += 1\n",
    "        \n",
    "    # for each project, get word count of each unique word\n",
    "    for j in count_dict:\n",
    "        count_dict[j] = count_dict[j] + ([0] * len(all_words))\n",
    "    \n",
    "    for i in range(len(all_words)):\n",
    "        for word in all_words[i]:\n",
    "            count_dict[word][i+1] += 1\n",
    "    \n",
    "    for key in count_dict:\n",
    "        assert (count_dict[key][0] == sum(count_dict[key][1:])), print('this key,', key, 'is weird.')\n",
    "\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def fv_dict_to_df(df_neuro):\n",
    "    \"\"\"\n",
    "    converts word_count_dict feature vectors into a dataframe with appropriate indices\n",
    "    \"\"\"\n",
    "    word_count_dict = unique_words(df_neuro)\n",
    "    \n",
    "    feature_vec = pd.DataFrame(word_count_dict)\n",
    "    fvec_index = ['Total'] + df_neuro.index.copy().tolist()\n",
    "    feature_vec.index = fvec_index\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Compute tf-idf transformation vector and get td-idf transformation of feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_idf(feature_vector):\n",
    "    \"\"\"\n",
    "    input: dataframe containing feature vectors where df.columns = words, and df.index is NIH funded project, and data contains count of word occurrences\n",
    "    returns: numpy array for idf part\n",
    "    \"\"\"\n",
    "    fv_no_tot = feature_vector.iloc[1:]\n",
    "    fv_no_tot = fv_no_tot.replace(0, np.nan)\n",
    "    \n",
    "    # get idf = log (number of documents/number of documents with term t in it)\n",
    "    return  np.log(len(feature_vector.iloc[0]) / (1 + np.array(fv_no_tot.count(axis=0))))\n",
    "def get_tfidf(feature_vector, df_neuro):\n",
    "    \"\"\"\n",
    "    feature_vector/total_words * idf\n",
    "    \"\"\"\n",
    "    fv_no_tot = feature_vector.iloc[1:]\n",
    "    \n",
    "    # get tf\n",
    "    total_words = df_neuro.ix[list(fv_no_tot.index), :].wordlist_ct\n",
    "    tf = fv_no_tot.div(total_words, axis=0)\n",
    "    \n",
    "    # get idf\n",
    "    idf = get_idf(feature_vector)\n",
    "    \n",
    "    # return td-idf\n",
    "    return (tf * idf).dropna()\n",
    "def clean_tfidf(tfidf):\n",
    "    tfidf_del = ['studi', 'cell', 'brain', 'neuron', 'use', 'determin', 'effect', 'function', 'respons', 'specif', \n",
    "             'propos', 'activ', 'investig', 'examin', 'system', 'techniqu', 'chang', 'experi', 'understand', 'may',\n",
    "            'aim']\n",
    "    for i in tfidf_del:\n",
    "        try:\n",
    "            del tfidf[i]\n",
    "        except:\n",
    "            pass\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decompose tf-idf using SVD (latent semantic analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_svd(tfidf):\n",
    "    \"\"\"\n",
    "    apply SVD to tfidf\n",
    "    \"\"\"\n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    \n",
    "    svd = TruncatedSVD()\n",
    "    svd.fit(tfidf)\n",
    "    pyplot.plot(svd.explained_variance_ratio_.cumsum())\n",
    "\n",
    "def recalc_svd(tfidf, components):\n",
    "    \"\"\"\n",
    "    get user input for number of components that should explain >0.9 variance\n",
    "    \"\"\"\n",
    "    \n",
    "    from sklearn.decomposition import TruncatedSVD\n",
    "    \n",
    "    svd = TruncatedSVD(n_components = components)\n",
    "    train_lsa = svd.fit_transform(tfidf)\n",
    "    return train_lsa, svd\n",
    "\n",
    "def plot_svd(svd, tfidf):\n",
    "    \"\"\"\n",
    "    plots word contribution for top 10 SVD components, and variance explained\n",
    "    \"\"\"\n",
    "    feat_names = list(tfidf.columns)\n",
    "\n",
    "    for compNum in range(0, 10):\n",
    "        comp = svd.components_[compNum]\n",
    "\n",
    "        # Sort the weights in the first component, and get the indices\n",
    "        indices = np.argsort(comp).tolist()\n",
    "\n",
    "        # Reverse the indices, so we have the largest weights first.\n",
    "        indices.reverse()\n",
    "\n",
    "        # Grab the top 10 terms which have the highest weight in this component.        \n",
    "        terms = [feat_names[weightIndex] for weightIndex in indices[0:10]]    \n",
    "        weights = [comp[weightIndex] for weightIndex in indices[0:10]]    \n",
    "\n",
    "        # Display these terms and their weights as a horizontal bar graph.    \n",
    "        # The horizontal bar graph displays the first item on the bottom; reverse\n",
    "        # the order of the terms so the biggest one is on top.\n",
    "        terms.reverse()\n",
    "        weights.reverse()\n",
    "        positions = np.arange(10) + .5    # the bar centers on the y axis\n",
    "\n",
    "        pyplot.figure(compNum)\n",
    "        pyplot.barh(positions, weights, align='center')\n",
    "        pyplot.yticks(positions, terms)\n",
    "        pyplot.xlabel('Weight')\n",
    "        pyplot.title('Strongest terms for component %d' % (compNum))\n",
    "        pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Get cosine similarity distance matrix\n",
    "\n",
    "Cosine similarity is measured against the tf-idf matrix and can be used to generate a measure of similarity between each document and the other documents in the corpus (each synopsis among the synopses). Subtracting it from 1 provides cosine distance which I will use for plotting on a euclidean (2-dimensional) plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_cosine_matrix(tfidf_matrix):\n",
    "    \"\"\"\n",
    "    input: tfidf matrix (n_samples X m_words)\n",
    "    returns: distance matrix (n_samples X n_samples) (np ndarray)\n",
    "    \"\"\"\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    return (1 - cosine_similarity(tfidf_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Euclidean similarity distance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euc_dist_matrix(tfidf):\n",
    "    from sklearn.metrics.pairwise import euclidean_distances\n",
    "    return euclidean_distances(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Putting everything together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [
     4
    ],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_matrices(year, grant_type='R01'):\n",
    "    \"\"\"\n",
    "    input:\n",
    "        year: string -- year to analyze; [1985:2016] inclusive.\n",
    "        grant_type: string, what type of grant do you want to analyze? \n",
    "            Default = 'R01'\n",
    "            Possibilities = ['D43', 'D71', 'DP1', 'DP2', 'DP3', 'DP5', \n",
    "            'DP7', 'E11', 'F30', 'F31', 'F32', 'F99', 'FI2', 'G08',\n",
    "            'G11', 'G12', 'G13', 'G20', 'H25', 'H79', 'I01', 'I21',\n",
    "            'I50', 'IK1', 'IK2', 'IP1', 'IS1', 'K01', 'K02', 'K05',\n",
    "            'K06', 'K07', 'K08', 'K12', 'K18', 'K22', 'K23', 'K24',\n",
    "            'K25', 'K26', 'K43', 'K76', 'K99', 'KL2', 'N01', 'N02',\n",
    "            'N03', 'N43', 'N44', 'OT2', 'OT3', 'P01', 'P20', 'P2C',\n",
    "            'P30', 'P40', 'P41', 'P42', 'P50', 'P51', 'P60', 'R00',\n",
    "            'R01', 'R03', 'R13', 'R15', 'R18', 'R21', 'R24', 'R25',\n",
    "            'R28', 'R33', 'R34', 'R35', 'R36', 'R37', 'R41', 'R42',\n",
    "            'R43', 'R44', 'R49', 'R50', 'R56', 'R61', 'R90', 'RF1',\n",
    "            'RL5', 'RM1', 'S10', 'S21', 'SB1', 'SC1', 'SC2', 'SC3',\n",
    "            'T01', 'T03', 'T15', 'T32', 'T34', 'T35', 'T36', 'T37',\n",
    "            'T42', 'T90', 'TL1', 'TL4', 'U01', 'U10', 'U13', 'U17',\n",
    "            'U18', 'U19', 'U22', 'U24', 'U2C', 'U2G', 'U2R', 'U34',\n",
    "            'U36', 'U38', 'U41', 'U42', 'U44', 'U45', 'U48', 'U50',\n",
    "            'U51', 'U52', 'U54', 'U58', 'U59', 'U60', 'U61', 'U62',\n",
    "            'U66', 'U88', 'U90', 'UC2', 'UC4', 'UC7', 'UE1', 'UF1',\n",
    "            'UF2', 'UG1', 'UG3', 'UG4', 'UH2', 'UH3', 'UH4', 'UL1',\n",
    "            'UM1', 'UM2', 'US4', 'UT2', 'Y01', 'Z01', 'ZIA', 'ZIB',\n",
    "            'ZIC', 'ZID', 'ZIE', 'ZIF', 'ZIG', 'ZIH', 'ZII', 'ZIJ',\n",
    "            'ZIK']\n",
    "    returns: None\n",
    "    saves: \n",
    "        fv_neuro_tfidf: dataframe\n",
    "        cos_matrix: np nd array\n",
    "        df_neuro_granttype: dataframe\n",
    "        as csv files to 'matrices' directory\n",
    "    \"\"\"\n",
    "    # get abstracts and projects for a single year\n",
    "    project, abstract = get_abstracts_projects(year)\n",
    "    \n",
    "    # join abstracts to projects dataframe\n",
    "    df = project.merge(abstract, on='APPLICATION_ID', how='left')\n",
    "    \n",
    "    # Implement clean abstracts and filter for neuro projects.\n",
    "    df_neuro = neuro_only(df)\n",
    "    \n",
    "    # look only at R01s\n",
    "    df_neuro_granttype = df_neuro[df_neuro.ACTIVITY == grant_type]\n",
    "    \n",
    "    # make word count feature vector\n",
    "    fv_neuro = fv_dict_to_df(df_neuro_granttype)\n",
    "    \n",
    "    # get tf-idf matrix\n",
    "    fv_neuro_tfidf = get_tfidf(fv_neuro, df_neuro)\n",
    "    \n",
    "    # clean tfidf\n",
    "    fv_neuro_tfidf = clean_tfidf(fv_neuro_tfidf)\n",
    "    \n",
    "    # get tfidf SVD \n",
    "    print (get_svd(fv_neuro_tfidf))\n",
    "    components = int(input('How many components to grab?: '))\n",
    "    tfidf_svd, svd = recalc_svd(fv_neuro_tfidf, components)\n",
    "    plot_svd(svd, fv_neuro_tfidf)\n",
    "    \n",
    "    # get cosine distance matrix\n",
    "    cos_matrix = get_cosine_matrix(tfidf_svd)\n",
    "    \n",
    "    # get euclidean distance matrix\n",
    "    euc_dist = euc_dist_matrix(tfidf_svd)\n",
    "    \n",
    "    # save files:\n",
    "    # save neuro R01s dataframe\n",
    "    df_neuro_granttype.to_csv('./matrices/neuro_df' + year + '_' + grant_type + '.csv')\n",
    "    \n",
    "    # save decomposed tfidf\n",
    "    tfidf_svd.to_csv('./matrices/tfidfsvd_' + year + '_' + grant_type + '.csv')\n",
    "    \n",
    "    # save cosine distance matrix (using decomposed tfidf)\n",
    "    np.savetxt('./matrices/cos_' + year + '_' + grant_type + '.csv', cos_matrix, delimiter = ',')\n",
    "    \n",
    "    # save euclidean distance matrix (using decomposed tfidf)\n",
    "    np.savetxt('./matrices/cos_' + year + '_' + grant_type + '.csv', euc_dist, delimiter = ',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/IPython/core/interactiveshell.py:2885: DtypeWarning: Columns (5,6,7,10,11,13,17,19,21,24,25,29,30,31,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n"
     ]
    }
   ],
   "source": [
    "# make folder to contain matrices\n",
    "try: \n",
    "    os.mkdir('./matrices/')\n",
    "except FileExistsError:\n",
    "    pass\n",
    "        \n",
    "# years = range(1985, 2017)\n",
    "years = range(2005, 2006)\n",
    "for year in years:\n",
    "    get_matrices(str(year))\n",
    "    print (str(year), 'data is done!')\n",
    "print ('all done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
