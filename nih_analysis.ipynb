{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import math\n",
    "\n",
    "from string import punctuation\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import PorterStemmer\n",
    "from nltk import FreqDist\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read abstracts and project csvs into dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "abs_dir = './abstracts/'\n",
    "proj_dir = './projects/'\n",
    "project = pd.read_csv(proj_dir + 'RePORTER_PRJ_C_FY2016.csv', \n",
    "                   encoding = \"ISO-8859-1\")\n",
    "abstract = pd.read_csv(abs_dir + 'RePORTER_PRJABS_C_FY2016.csv',\n",
    "                      encoding = \"ISO-8859-1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Clustering NIH data by common keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# join abstracts to projects dataframe\n",
    "df = project.merge(abstract, on='APPLICATION_ID', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Clean abstracts (stemming, lowercase, no punctuation, remove stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# get customized stopwords\n",
    "to_append = []\n",
    "\n",
    "def customized_stopwords(to_append):\n",
    "    \"\"\"\n",
    "    to_append: list; what words do you want to exclude from your analysis, in addition to the standard \n",
    "    stopwords like 'the', 'and, 'of', and so on? See above to_append variable for examples.\n",
    "    returns: list; stopwords including to_append list\n",
    "    \"\"\"\n",
    "    stop = stopwords.words('english')\n",
    "    stop = stop + to_append\n",
    "    return stop\n",
    "\n",
    "stop = customized_stopwords(to_append)\n",
    "\n",
    "# get list of words that are lowercase, with punctuation removed and words stemmed.\n",
    "def get_wordlist(abstract):\n",
    "    \"\"\"\n",
    "    returns a list of lowercase words from abstract with punctuation and stopwords removed.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # make words lowercase\n",
    "        words = abstract.lower()\n",
    "        \n",
    "        # take out all punctuation and split strings into a list of words\n",
    "        words = (''.join(c for c in words if c not in punctuation)).split(' ')\n",
    "        \n",
    "        # remove stopwords\n",
    "        words = [\" \".join([w for w in word.split() if not w in stop]) for word in words]\n",
    "        \n",
    "        # stem words using Porter's Stemmer\n",
    "        stemmed = []\n",
    "        for word in words:\n",
    "            try:\n",
    "                word = PorterStemmer().stem(word)\n",
    "            except IndexError:\n",
    "                word = word\n",
    "            if word != '' and word.isalpha():\n",
    "                stemmed.append(word)\n",
    "        words = stemmed\n",
    "\n",
    "    except AttributeError:\n",
    "        words = []\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Filter for neuro-related abstracts.\n",
    "I defined a project to be neuroscience-related if the abstract mentioned \"brain\" or \"neur*\" at least once every 100 non-stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def neuro_count(row):\n",
    "    try:\n",
    "        return (row.ABSTRACT_TEXT.count(' brain') + row.ABSTRACT_TEXT.count('neur'))\n",
    "    except AttributeError:\n",
    "        return 0\n",
    "    \n",
    "def wordlist_count(row):\n",
    "    return len(row.wordlist)\n",
    "\n",
    "def neuro_only(df, word_density=0.01):\n",
    "    \"\"\"\n",
    "    input: dataframe\n",
    "    word_density: how many neuro related words for every 100 words that are not stopwords in an abstract? Stopwords: the, and, or, not, etc.\n",
    "    returns: dataframe containing neuro-related projects as defined above, with a column containing cleaned abstract keywords for analysis.\n",
    "    \"\"\"\n",
    "    df['abs_neuro_count'] = df.apply(neuro_count, axis=1)\n",
    "    df['wordlist'] = df.ABSTRACT_TEXT.apply(get_wordlist)\n",
    "    df['wordlist_ct'] = df.apply(wordlist_count, axis=1)\n",
    "    df['rel_neuro_count'] = df.abs_neuro_count / df.wordlist_ct\n",
    "    return df[df.rel_neuro_count >= word_density]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement clean abstracts and filter for neuro projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:16: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "//anaconda/lib/python3.5/site-packages/ipykernel/__main__.py:19: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "df_100 = df.ix[:100]\n",
    "df_neuro = neuro_only(df_100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get feature vectors into a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def unique_words(df):\n",
    "    \"\"\"\n",
    "    input: df containing 'wordlist' column\n",
    "    returns: a dictionary containing unique words as keys, where dict[key][0] is total count of word over df\n",
    "    \"\"\"\n",
    "    all_words = list(df.wordlist)\n",
    "    all_words_list = [item for sublist in all_words for item in sublist]\n",
    "\n",
    "    count_dict = {}\n",
    "    \n",
    "    # Get total word count for each word in dataset\n",
    "    for word in all_words_list:\n",
    "        if word not in count_dict.keys():\n",
    "            count_dict[word] = [1]\n",
    "        else:\n",
    "            count_dict[word][0] += 1\n",
    "        \n",
    "    # for each project, get word count of each unique word\n",
    "    for j in count_dict:\n",
    "        count_dict[j] = count_dict[j] + ([0] * len(all_words))\n",
    "    \n",
    "    for i in range(len(all_words)):\n",
    "        for word in all_words[i]:\n",
    "            count_dict[word][i+1] += 1\n",
    "    \n",
    "    for key in count_dict:\n",
    "        assert (count_dict[key][0] == sum(count_dict[key][1:])), print('this key,', key, 'is weird.')\n",
    "\n",
    "    return count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fv_dict_to_df(df_neuro):\n",
    "    \"\"\"\n",
    "    converts word_count_dict feature vectors into a dataframe with appropriate indices\n",
    "    \"\"\"\n",
    "    word_count_dict = unique_words(df_neuro)\n",
    "    \n",
    "    feature_vec = pd.DataFrame(word_count_dict)\n",
    "    fvec_index = ['Total'] + df_neuro.index.copy().tolist()\n",
    "    feature_vec.index = fvec_index\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fv_neuro = fv_dict_to_df(df_neuro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute tf-idf transformation vector and get td-idf transformation of feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": [],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_idf(feature_vector):\n",
    "    \"\"\"\n",
    "    input: dataframe containing feature vectors where df.columns = words, and df.index is NIH funded project, and data contains count of word occurrences\n",
    "    returns: numpy array for idf part\n",
    "    \"\"\"\n",
    "    fv_no_tot = feature_vector.iloc[1:]\n",
    "    fv_no_tot = fv_no_tot.replace(0, np.nan)\n",
    "    \n",
    "    # get idf = log (number of documents/number of documents with term t in it)\n",
    "    return  np.log(len(feature_vector.iloc[0]) / (1 + np.array(fv_no_tot.count(axis=0))))\n",
    "\n",
    "def get_tfidf(feature_vector, df_neuro):\n",
    "    \"\"\"\n",
    "    feature_vector/total_words * idf\n",
    "    \"\"\"\n",
    "    fv_no_tot = feature_vector.iloc[1:]\n",
    "    \n",
    "    # get tf\n",
    "    total_words = df_neuro.ix[list(fv_no_tot.index), :].wordlist_ct\n",
    "    tf = fv_no_tot.div(total_words, axis=0)\n",
    "    \n",
    "    # get idf\n",
    "    idf = get_idf(feature_vector)\n",
    "    \n",
    "    # return td-idf\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fv_neuro_tfidf = get_tfidf(fv_neuro, df_neuro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other analyses (grant funding and number, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable               Type                    Data/Info\n",
      "--------------------------------------------------------\n",
      "FreqDist               type                    <class 'nltk.probability.FreqDist'>\n",
      "PorterStemmer          type                    <class 'nltk.stem.porter.PorterStemmer'>\n",
      "TfidfVectorizer        type                    <class 'sklearn.feature_e<...>on.text.TfidfVectorizer'>\n",
      "abs_dir                str                     ./abstracts/\n",
      "abstract               DataFrame                      APPLICATION_ID    <...>n[70512 rows x 2 columns]\n",
      "customized_stopwords   function                <function customized_stopwords at 0x115c9c2f0>\n",
      "df                     DataFrame                      APPLICATION_ID ACT<...>[71827 rows x 46 columns]\n",
      "df_100                 DataFrame                    APPLICATION_ID ACTIV<...>\\n[101 rows x 50 columns]\n",
      "df_neuro               DataFrame                   APPLICATION_ID ACTIVI<...>n\\n[18 rows x 50 columns]\n",
      "fv_dict_to_df          function                <function fv_dict_to_df at 0x12672ac80>\n",
      "fv_neuro               DataFrame                      abil  ablat  abnor<...>n[19 rows x 1453 columns]\n",
      "fv_neuro_tfidf         DataFrame                       abil     ablat   <...>n[18 rows x 1453 columns]\n",
      "get_idf                function                <function get_idf at 0x12672ab70>\n",
      "get_tfidf              function                <function get_tfidf at 0x12672aae8>\n",
      "get_wordlist           function                <function get_wordlist at 0x12672af28>\n",
      "math                   module                  <module 'math' from '//an<...>3.5/lib-dynload/math.so'>\n",
      "neuro_count            function                <function neuro_count at 0x12672a950>\n",
      "neuro_only             function                <function neuro_only at 0x11605d730>\n",
      "np                     module                  <module 'numpy' from '//a<...>kages/numpy/__init__.py'>\n",
      "os                     module                  <module 'os' from '//anac<...>nda/lib/python3.5/os.py'>\n",
      "pd                     module                  <module 'pandas' from '//<...>ages/pandas/__init__.py'>\n",
      "proj_dir               str                     ./projects/\n",
      "project                DataFrame                      APPLICATION_ID ACT<...>[71827 rows x 45 columns]\n",
      "punctuation            str                     !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "sns                    module                  <module 'seaborn' from '/<...>ges/seaborn/__init__.py'>\n",
      "stop                   list                    n=153\n",
      "stopwords              WordListCorpusReader    <WordListCorpusReader in <...>_data/corpora/stopwords'>\n",
      "to_append              list                    n=0\n",
      "unique_words           function                <function unique_words at 0x11605d9d8>\n",
      "wordlist_count         function                <function wordlist_count at 0x11605d598>\n"
     ]
    }
   ],
   "source": [
    "%whos"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
